# -*- coding: utf-8 -*-
"""project 2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ISLab4gaqs_8EZrbuXM4P6rBeM557lBZ
"""

# Commented out IPython magic to ensure Python compatibility.
# %load_ext tensorboard

# Commented out IPython magic to ensure Python compatibility.
# %reload_ext tensorboard

import os
import re
import nltk
import string
import datetime
import numpy as np
import pandas as pd
import tensorflow as tf
import matplotlib.pyplot as plt
import sklearn, datetime, pickle

from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
from nltk.tokenize import word_tokenize
from sklearn import preprocessing, model_selection, metrics
from keras import layers, losses, metrics, activations, callbacks, initializers, regularizers, optimizers, Input,Sequential

from google.colab import files
uploaded = files.upload()

data = pd.read_csv('ecommerceDataset.csv', names=['product', 'description'])

data.info()

data.head()

print("number of catgories: ", len(data['product'].unique()))

print("Categories:\n", data['product'].value_counts())

print("Missing values:\n", data.isna().sum())

print("Duplicate:\n", data.duplicated().sum())

data = data.dropna(subset=['description'])

print("Missing values:\n", data.isna().sum())

data = data.dropna(subset=['description'])

print("Missing values:\n", data.isna().sum())

print("Categories:\n", data['product'].value_counts())

stemmer = PorterStemmer()

nltk.download('stopwords')
nltk.download('punkt')

nltk.download('punkt_tab')

stop_words = set(stopwords.words('english'))

def clean_text(new_data):
    new_data = re.sub(r'http\S+|www\.\S+', '', new_data)
    new_data = re.sub(r'[^a-zA-Z\s]', '', new_data)
    new_data = re.sub(r'\s+', ' ', new_data).strip()
    new_data = re.sub(r'</br>', ' ', new_data).strip()
    tokens = word_tokenize(new_data)
    new_data = ' '.join(stemmer.stem(word) for word in tokens if word not in stop_words)
    return new_data

data['description'] = data['description'].apply(clean_text)

label_encoder = sklearn.preprocessing.LabelEncoder()
data['product'] = label_encoder.fit_transform(data['product'])
print(data)

features = data['description']
label = data['product']

sample_categories = [0, 1, 2, 3]
print(label_encoder.inverse_transform(sample_categories))

#0 - Books
#1 - Clothing & Accessories
#2 - Electronics
#3 - Household

SEED = 42

X_train, X_split, y_train, y_split = model_selection.train_test_split(features, label, train_size=0.7, random_state=SEED)

X_val, X_test, y_val, y_test = model_selection.train_test_split(X_split, y_split, train_size=0.5, random_state=SEED)

y_val.head()

print(X_test)

input_length=50
tokenizer = tf.keras.layers.TextVectorization(max_tokens=3000, output_sequence_length=input_length)
tokenizer.adapt(X_train)

X_train.head()

sample_text = X_train[:2]
sample_tokens = tokenizer(sample_text)
print(sample_text)
print(sample_tokens)

X_train = tokenizer(X_train)
X_test = tokenizer(X_test)
X_val = tokenizer(X_val)

print(X_val)

embedding = layers.Embedding(3000, 64)

l1 = regularizers.L1(l1=0.01)
l2 = regularizers.L2(l2=0.01)
l1l2 = regularizers.L1L2(l1=0.005,l2=0.01)

model = Sequential()

model.add(layers.Input(shape=(input_length,)))

#nlp layer
model.add(embedding)

model.add(layers.SpatialDropout1D(0.2))
model.add(layers.Conv1D(64, 3, activation='relu'))

#rnn layer
model.add(layers.Bidirectional(layers.LSTM(32, dropout=0.2, recurrent_dropout=0.2)))
model.add(layers.Dropout(0.3))

#hidden layer
model.add(layers.Dense(16, activation='relu', kernel_regularizer=l2))

#output layer
model.add(layers.Dense(len(data['product'].unique()), activation="softmax"))

model.summary()

model.compile(optimizer="adam", loss='sparse_categorical_crossentropy', metrics=['accuracy'])

log_dir = "logs/fit/" + datetime.datetime.now().strftime("%Y%m%d-%H%M%S")
tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)

# Commented out IPython magic to ensure Python compatibility.
# %tensorboard --logdir logs

history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=10, batch_size=32, callbacks=[tensorboard_callback])

from tensorboard import notebook
notebook.list()

notebook.display(port=6006, height=1000)

plt.figure(figsize=(7,7))
plt.plot(history.epoch,history.history['loss'])
plt.plot(history.epoch,history.history['val_loss'])
plt.legend(['Training loss', 'validation loss'])
plt.show()

print(model.evaluate(X_test, y_test))

y_pred = model.predict(X_test)

print(y_pred)

y_pred = np.argmax(y_pred, axis=1)

print(y_pred)

print(y_test)

y_test.shape

y_pred.shape

y_pred = model.predict(X_test)
y_pred = np.argmax(y_pred, axis=1)

print('F1 score:', sklearn.metrics.f1_score(y_test, y_pred, average='weighted'))

model.save("nlp_model.h5")
